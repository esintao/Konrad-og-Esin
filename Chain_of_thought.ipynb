{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsycU8Sbj4B9PUxOf5QB+u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esintao/Konrad-og-Esin/blob/main/Chain_of_thought.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DHFwdkX2McCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e78fb9-0793-4dfe-8c87-28ac19738f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "Hb5VBBORMw_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email about an alpaca that likes flan\"\n",
        "\n",
        "model = pipeline(task=\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "model(prompt, max_length=128, do_sample=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aONQLLOkMkBn",
        "outputId": "7b7b70a2-391e-4944-d87d-59913fc3f332"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Alpaca: I have an alpaca that likes flan. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he's a very small one. He's a very small one and he'\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "W6UXmWNOMzTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Jp08Zz7cOeHD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"derek-thomas/ScienceQA\")\n",
        "\n",
        "df = pd.DataFrame.from_dict(dataset['train'])\n",
        "\n",
        "df = df[(df['choices'].str.len() == 4) & (df['image'].isna())]\n",
        "\n",
        "df = df.sample(n=200)\n",
        "\n"
      ],
      "metadata": {
        "id": "-3vpWHVWMweN"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting without CoT"
      ],
      "metadata": {
        "id": "Zz27mtA7Wwzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers = []\n",
        "question_list = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    question_text = (\n",
        "        f\"Question: {row['question']}\\n\"\n",
        "        f\"(A) {row['choices'][0]} \"\n",
        "        f\"(B) {row['choices'][1]} \"\n",
        "        f\"(C) {row['choices'][2]} \"\n",
        "        f\"(D) {row['choices'][3]}\"\n",
        "        f\"The answer must be formatted as 1 letter, A-D.\"\n",
        "    )\n",
        "\n",
        "    answers.append(model(question_text, max_new_tokens=1, do_sample=False))\n",
        "    question_list.append(question_text)\n",
        "\n",
        "answers[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1tVp7cuSzeJ",
        "outputId": "6d61a9b3-3c3c-49b3-ef57-56295c1e55db"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'A'}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UXi7XtRaU6IG",
        "outputId": "65484f32-56e9-4130-d35f-a0833f521765"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Question: How long is a rowboat?\\n(A) 3 centimeters (B) 3 kilometers (C) 3 meters (D) 3 millimetersThe answer must be formatted as 1 letter, A-D.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting with CoT"
      ],
      "metadata": {
        "id": "4lkvFKnyW0xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers_CoT = []\n",
        "question_list_CoT = []\n",
        "\n",
        "\n",
        "# 1. Define a template with a \"Few-Shot\" example\n",
        "FEW_SHOT_EXAMPLE = \"\"\"You are a careful reasoning assistant that answers multiple-choice questions about science.\n",
        "First, think step by step. Then, give the final answer.\n",
        "\n",
        "Example Question: What is the main source of energy for Earth?\n",
        "(A) The Moon (B) The Sun (C) Burning coal (D) Wind\n",
        "Reasoning: Step 1: Earth's climate and life processes require a massive constant energy source.\n",
        "Reasoning: Step 2: While wind and coal provide energy, they are secondary sources.\n",
        "Reasoning: Step 3: The Sun provides the vast majority of energy to Earth via radiation.\n",
        "Answer: B\n",
        "\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "answers_CoT = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    # 2. Construct the prompt with clear newlines and the example\n",
        "    prompt = (\n",
        "        f\"{FEW_SHOT_EXAMPLE}\\n\"\n",
        "        f\"Question: {row['question']}\\n\"\n",
        "        f\"(A) {row['choices'][0]}\\n\"\n",
        "        f\"(B) {row['choices'][1]}\\n\"\n",
        "        f\"(C) {row['choices'][2]}\\n\"\n",
        "        f\"(D) {row['choices'][3]}\\n\"\n",
        "        f\"Format exactly as:\\n\"\n",
        "        f\"Reasoning: <step 1>\\n\"\n",
        "        f\"Reasoning: <step 2>\\n\"\n",
        "        f\"Reasoning: <step 3>\\n\"\n",
        "        f\"Answer: <final letter>\\n\"\n",
        "        f\"Reasoning:\" # This \"nudge\" forces the model to start with the correct word\n",
        "    )\n",
        "\n",
        "    # 3. Generate with greedy decoding (do_sample=False) for better formatting consistency\n",
        "    output = model(prompt, max_new_tokens=256, do_sample=False)\n",
        "\n",
        "    # We prepend \"Reasoning:\" back to the text since we used it as a nudge\n",
        "    generated_text = \"Reasoning:\" + output[0]['generated_text']\n",
        "    answers_CoT.append(generated_text)\n",
        "    question_list_CoT.append(prompt)\n",
        "\n",
        "answers_CoT[0]"
      ],
      "metadata": {
        "id": "DErQ6MmpfQgc",
        "outputId": "1fdc0904-ea49-4740-83a2-df9823d2f3d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Reasoning:A cookie is not like the others. The final answer: A ---'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_list_CoT[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "w0uIau8LXa2i",
        "outputId": "4c3e86a8-523f-4762-9451-6f99eee1d637"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a careful reasoning assistant that answers multiple-choice questions about science. First, think step by step. Then, give the final answer. You must format exactly as:\\nReasoning: <step 1>\\nReasoning: <step 2>\\nReasoning: <step 3>\\nAnswer: <final letter>\\n\\nQuestion: Which word is not like the others?\\n(A) cookie\\n(B) ice cream\\n(C) cake\\n(D) dirt\\nNow answer using the format.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}