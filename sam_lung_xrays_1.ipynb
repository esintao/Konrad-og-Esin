{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f400486b",
      "metadata": {
        "id": "f400486b"
      },
      "source": [
        "# Segmenting Lung X-ray Images with the Segment Anything Model\n",
        "### Advanced Deep Learning 2022\n",
        "Notebook written by [Jakob Ambsdorf](mailto:jaam@di.ku.dk).\n",
        "Lung x-ray code originally written by Mathias Perslev. It has been changed slightly by Christian Igel and subsequently slightly updated [Stefan Sommer](mailto:sommer@di.ku.dk).\n",
        "SAM related code (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "We consider the data described in:\n",
        "Bram van Ginneken, Mikkel B. Stegmann, Marco Loog. [Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database](https://doi.org/10.1016/j.media.2005.02.002). *Medical Image Analysis* 10(1): 19-40, 2006"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ae39ff",
      "metadata": {
        "id": "a1ae39ff"
      },
      "source": [
        "## Object masks from prompts with SAM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a4b25c",
      "metadata": {
        "id": "b4a4b25c"
      },
      "source": [
        "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt.\n",
        "\n",
        "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644532a8",
      "metadata": {
        "id": "644532a8"
      },
      "source": [
        "## Environment Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fabfee",
      "metadata": {
        "id": "07fabfee"
      },
      "source": [
        "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5ea65efc",
      "metadata": {
        "id": "5ea65efc"
      },
      "outputs": [],
      "source": [
        "using_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91dd9a89",
      "metadata": {
        "id": "91dd9a89",
        "outputId": "c21c21c0-2f50-4b1b-8699-7bb85990c57c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "Torchvision version: 0.24.0+cu126\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-zbcodxzv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-zbcodxzv\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment_anything\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=6f9f3adbfda076d5c8dcf340d111763bec926a3b3d12f8a58b0eafd7cea50931\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-csd7dzfe/wheels/29/82/ff/04e2be9805a1cb48bec0b85b5a6da6b63f647645750a0e42d4\n",
            "Successfully built segment_anything\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n",
            "--2025-12-15 14:53:25--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 271475 (265K) [image/jpeg]\n",
            "Saving to: ‘images/truck.jpg’\n",
            "\n",
            "truck.jpg           100%[===================>] 265.11K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-12-15 14:53:25 (12.3 MB/s) - ‘images/truck.jpg’ saved [271475/271475]\n",
            "\n",
            "--2025-12-15 14:53:25--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168066 (164K) [image/jpeg]\n",
            "Saving to: ‘images/groceries.jpg’\n",
            "\n",
            "groceries.jpg       100%[===================>] 164.13K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-12-15 14:53:25 (11.8 MB/s) - ‘images/groceries.jpg’ saved [168066/168066]\n",
            "\n",
            "--2025-12-15 14:53:25--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.41.33, 99.84.41.79, 99.84.41.129, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.41.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   140MB/s    in 17s     \n",
            "\n",
            "2025-12-15 14:53:42 (145 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir images\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be845da",
      "metadata": {
        "id": "0be845da"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33681dd1",
      "metadata": {
        "id": "33681dd1"
      },
      "source": [
        "Necessary imports and helper functions for displaying points, boxes, and masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "69b28288",
      "metadata": {
        "id": "69b28288"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "29bc90d5",
      "metadata": {
        "id": "29bc90d5"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ca884f",
      "metadata": {
        "id": "06ca884f"
      },
      "source": [
        "# Download model checkpoint\n",
        "The checkpoint is 2.39GB, takes a few minutes for most bandwidths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7410dac",
      "metadata": {
        "id": "a7410dac",
        "outputId": "ecf84cb6-9717-49e3-9568-e10a4e862fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sam_vit_h_4b8939.pth:   9%|█▋                | 229M/2.39G [00:01<00:11, 202MB/s]"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
        "filename = \"sam_vit_h_4b8939.pth\"\n",
        "folder = \"models\"\n",
        "\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "filepath = os.path.join(folder, filename)\n",
        "\n",
        "if not os.path.exists(filepath):\n",
        "    # Get the file size before downloading\n",
        "    file_size = int(urllib.request.urlopen(url).info().get(\"Content-Length\", -1))\n",
        "\n",
        "    # Start the download with progress bar\n",
        "    with tqdm(unit=\"B\", unit_scale=True, unit_divisor=1024, total=file_size, desc=filename, ncols=80) as pbar:\n",
        "        urllib.request.urlretrieve(url, filepath, reporthook=lambda b, bsize, t: pbar.update(bsize))\n",
        "else:\n",
        "    print(\"Checkpoint file already exists. Skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e600bc05",
      "metadata": {
        "id": "e600bc05"
      },
      "outputs": [],
      "source": [
        "#import sys\n",
        "#sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"models/sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VkTX9wam7nhj",
      "metadata": {
        "id": "VkTX9wam7nhj"
      },
      "source": [
        "# Chest X-ray Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l0RJqwoq8kHm",
      "metadata": {
        "id": "l0RJqwoq8kHm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.datasets.utils import download_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LhXWkNR47rxA",
      "metadata": {
        "id": "LhXWkNR47rxA"
      },
      "outputs": [],
      "source": [
        "# Mount Google drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive/')\n",
        "    os.chdir('gdrive/MyDrive/ADL2022')\n",
        "except:\n",
        "    print('Google drive not mounted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d5b014",
      "metadata": {
        "id": "f3d5b014"
      },
      "outputs": [],
      "source": [
        "# If you are getting a download error, comment in the following lines:\n",
        "# import ssl\n",
        "# ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ET9ewnZ_8N7t",
      "metadata": {
        "id": "ET9ewnZ_8N7t"
      },
      "outputs": [],
      "source": [
        "# Load database with chest X-rays with lung segmentations.\n",
        "data_root='./datasets'\n",
        "data_npz='lung_field_dataset.npz'\n",
        "data_fn = os.path.join(data_root, \"lung_field_dataset.npz\")\n",
        "force_download = False\n",
        "\n",
        "if (not os.path.exists(data_fn)) or force_download:\n",
        "    download_url(\"https://sid.erda.dk/share_redirect/gCTc6o3KAh\", data_root, data_npz)\n",
        "else:\n",
        "    print('Using existing', data_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MCnFSLuY5cMZ",
      "metadata": {
        "id": "MCnFSLuY5cMZ"
      },
      "outputs": [],
      "source": [
        "def plot_image_with_segmentation(image, segmentation, ax=None):\n",
        "    \"\"\"\n",
        "    Plots an image with overlayed segmentation mask\n",
        "\n",
        "    Returns: plt.fig and ax objects\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
        "    mask = np.ma.masked_where(segmentation == 0, segmentation)\n",
        "    ax.imshow(mask.squeeze(), cmap=\"Set1\", alpha=0.5)\n",
        "    return plt.gcf(), ax\n",
        "\n",
        "\n",
        "def load_npz_dataset(path, keys=('x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test')):\n",
        "    archive = np.load(path)\n",
        "    return [archive.get(key) for key in keys]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a475f922",
      "metadata": {
        "id": "a475f922"
      },
      "outputs": [],
      "source": [
        "def map_interval(image, from_min, from_max, to_min, to_max):\n",
        "    \"\"\"\n",
        "    Map values from [from_min, from_max] to [to_min, to_max]\n",
        "    \"\"\"\n",
        "    from_range = from_max - from_min\n",
        "    to_range = to_max - to_min\n",
        "    # scaled = np.array((image - from_min) / float(from_range), dtype=float)\n",
        "    scaled = (image - from_min) / float(from_range)\n",
        "    return to_min + (scaled * to_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aI_AlP5V5p8-",
      "metadata": {
        "id": "aI_AlP5V5p8-"
      },
      "outputs": [],
      "source": [
        "# Load train/val/test data\n",
        "x_train, y_train, x_val, y_val, x_test, y_test = load_npz_dataset(data_fn)\n",
        "\n",
        "# TODO:\n",
        "# Bring images into the correct format for SAM:\n",
        "# Image shape: (N, H, W, C=3)\n",
        "# Mask shape: (N, H, W)\n",
        "# Values: [0, 255] (uint8)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# your data should pass the following asserts\n",
        "assert x_train.shape == (112, 256, 256, 3)\n",
        "assert y_train.shape == (112, 256, 256)\n",
        "assert x_val.shape == (12, 256, 256, 3)\n",
        "assert y_val.shape == (12, 256, 256)\n",
        "assert x_test.shape == (123, 256, 256, 3)\n",
        "assert y_test.shape == (123, 256, 256)\n",
        "\n",
        "assert x_train.dtype == y_train.dtype == np.uint8\n",
        "assert np.min(x_train) == 0\n",
        "assert np.max(x_train) == 255\n",
        "\n",
        "# Plot an example\n",
        "fig, ax = plot_image_with_segmentation(x_train[0], y_train[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9yHsXs2QugdE",
      "metadata": {
        "id": "9yHsXs2QugdE"
      },
      "source": [
        "# Single Example image\n",
        "\n",
        "Let's try to run SAM on a single example image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m6tcM2rQs_Hj",
      "metadata": {
        "id": "m6tcM2rQs_Hj"
      },
      "outputs": [],
      "source": [
        "example_img, example_mask = x_train[0], y_train[0]\n",
        "\n",
        "input_points = None # TODO: Pick apropriate input points,\n",
        "input_label = None # TODO: Pick apropriate input labels\n",
        "\n",
        "example_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BhM_AohdukYD",
      "metadata": {
        "id": "BhM_AohdukYD"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(example_img)\n",
        "show_points(input_points, input_label, plt.gca()) # You may also use other prompt methods!\n",
        "plt.axis('on')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PS-UajooqKDN",
      "metadata": {
        "id": "PS-UajooqKDN"
      },
      "outputs": [],
      "source": [
        "predictor.set_image(example_img)\n",
        "\n",
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_points,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iE0pIkugqV-i",
      "metadata": {
        "id": "iE0pIkugqV-i"
      },
      "outputs": [],
      "source": [
        "# Note that the \"score\" here is an estimation of the mask quality, not the quality of the segmentation compared to the ground truth.\n",
        "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(example_img.squeeze())\n",
        "    show_mask(mask, plt.gca())\n",
        "    show_points(input_points, input_label, plt.gca())\n",
        "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e2bf35b",
      "metadata": {
        "id": "0e2bf35b"
      },
      "source": [
        "# Evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387a3d93",
      "metadata": {
        "id": "387a3d93"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "f1 = torchmetrics.F1Score(task=\"binary\")\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "for img, mask_gt in zip(x_val, y_val):\n",
        "    raise NotImplementedError(\"TODO: Predict segmentation with SAM and compute F1 score.\")\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "mean_f1 = None # TODO: Compute mean F1 score\n",
        "std_f1 = None # TODO: Compute standard deviation of F1 scores\n",
        "\n",
        "print(f\"Mean F1 score: {mean_f1:.4f}\")\n",
        "print(f\"Standard deviation: {std_f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}